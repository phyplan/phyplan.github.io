<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PhyPlan: Compositional and Adaptive Physical Task Reasoning with Physics-Informed Skill Networks for Robot Manipulators">
  <meta name="keywords" content="PhyPlan, PhyPlan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PhyPlan</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">PhyPlan: Compositional and Adaptive Physical Task Reasoning with Physics-Informed Skill Networks for Robot Manipulators</h1>
          <h3 class="title is-4 conference-authors"><a target="_blank" href="https://arxiv.org/">......</a></h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
               <sup>*1</sup>,</span>
            <span class="author-block">
              <sup>*1</sup>,</span>
            <span class="author-block">
              <sup>*1</sup>,</span>
            <span class="author-block">
              <sup>1</sup>,</span>
            <span class="author-block">
              <sup>#1</sup>,</span>
            <span class="author-block">
              <sup>#1</sup>,</span>
            <span class="author-block">
              <sup>1</sup>,</span>
            <span class="author-block">
              <sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup> </span>
            <span class="author-block"><sup>*</sup> and <sup>#</sup> </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2211.06652"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2211.06652"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://csciitd-my.sharepoint.com/:v:/g/personal/cs5180425_iitd_ac_in/ERlgDDEWUG1NqbseYkqz-CYBT_co1JVUdLd-qh9GjaISwg"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/dair-iitd/NSRM"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered is-vcentered">
        <div class="column is-half">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/v1.mp4"
                type="video/mp4">
      </video>
    </div>
    <div class="column is-one-quarter">
      <p class="has-text-centered">
        Put the white dice above the yellow lego object and move the yellow cube on top of the white dice
      <p>
    </div>
      </div>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">NSRM</span> is a novel neuro-symbolic model that learns the semantics of visual, action and spatial concepts in an end-to-end manner with no sub-goal supervision
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Given the task of positioning a ball-like object to a goal region beyond direct reach, humans can often throw, slide, or rebound objects against the wall to attain the goal. However, enabling robots to reason similarly is non-trivial. Existing methods for physical reasoning are data-hungry and struggle with complexity and uncertainty inherent in the real world. This paper presents PhyPlan, a novel physics-informed planning framework that combines physics-informed neural networks (PINNs) with modified Monte Carlo Tree Search (MCTS) to enable embodied agents to perform dynamic physical tasks. PhyPlan leverages PINNs to simulate and predict outcomes of actions in a fast and accurate manner and uses MCTS for planning. It dynamically determines whether to consult a PINN-based simulator (coarse but fast) or engage directly with the actual environment (fine but slow) to determine optimal policy. Evaluation with robots in simulated 3D environments demonstrates the ability of our approach to solve 3D-physical reasoning tasks involving the composition of dynamic skills. Quantitatively, PhyPlan excels in several aspects: (i) it achieves lower regret when learning novel tasks compared to state-of-the-art, (ii) it expedites skill learning and enhances the speed of physical reasoning, (iii) it demonstrates higher data efficiency compared to a physics un-informed approach. output a manipulation program that can be executed by the robot on the input scene resulting in the desired output scene.
          </p>
  
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/GTsjNbbw0-U" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Generalisation</h2>

    <div class="columns is-centered">

      <div class="column">
        <div class="content">
          <h3 class="title is-4">Richer scenes</h3>
          <p>
            Model is trained on simple scenes with 3-5 objects. It generalizes well to richer scenes with more objects, such as this scene with 15 objects.
          </p>
          <video id="dollyzoom" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/v2.mp4"
                    type="video/mp4">
          </video>
          <p class="teaser has-text-centered">
            Move the cyan cube on top of the red lego object
          </p>
        </div>
      </div>

      <div class="column">
        <h3 class="title is-4">Multi-step plans</h3>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              Model is trained on instructions up to 2 steps. It generalizes well to multi-step plans, such as this example with a 5-step instruction 
            </p>
            <video id="matting-video" autoplay muted loop playsinline height="100%">
              <source src="./static/videos/v3.mp4"
                      type="video/mp4">
            </video>
            <p class="teaser has-text-centered">
              Put the blue lego thing to the left of the red lego thing and place the red cube on the left side of the white lego object and move the magenta dice to the right of the green box and move the red box on the left side of the blue lego thing and put the blue lego object to the left of the white lego thing
            </p>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title">References</h2>
<p id="ref1">
  <sup>[1]</sup>R. Paul, J. Arkin, N. Roy, and T. M Howard, “Efficient grounding of abstract spatial concepts for natural language interaction with robot manipulators,” in Robotics: Science and Systems Foundation, 2016
</p>
<p id="ref2">
  <sup>[2]</sup>C. Paxton, Y. Bisk, J. Thomason, A. Byravan, and D. Fox, “Prospection: Interpretable plans from language by predicting the future,” in 2019 International Conference on Robotics and Automation (ICRA), IEEE, 2019, pp. 6942-6948
</p>
<p id="ref3">
  <sup>[3]</sup>M. Shridhar, L. Manuelli, and D. Fox, “Cliport: What and where pathways for robotic manipulation,” in Conference on Robot Learning, PMLR, 2022, pp. 894-906
</p>
</div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Citation</h2>
    <pre><code>@inproceedings{Kalithasan2023NSRM,
      title     = {Learning Neuro-symbolic Programs for Language Guided Robot Manipulation},
      author    = {Kalithasan, Namasivayam and Singh, Himanshu and Bindal, Vishal and Tuli, Arnav and Agrawal, Vishwajeet and Jain, Rahul and Singla, Parag and Paul, Rohan},
      booktitle = {IEEE International Conference on Robotics and Automation},
      year      = {2023}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
            Website template borrowed from <a
            href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
    </div>
  </div>
</footer>

</body>
</html>
